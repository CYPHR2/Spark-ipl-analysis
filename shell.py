var = spark.read.json("sample.json", multiLine = "true")
from pyspark.sql.functions import *
var = spark.read.json("sample.json", multiLine = "true")
var.printSchema()
var2 = var.select("innings")
var3 = var2.select("innings.overs")
var4 = var2.select(explode("innings.overs"))
var4.printSchema()
var5=var4.select("col.deliveries")
var5.printSchema()
var5.select("col.batter").show()
dataSet =var5.select("col.batter","col.bowler",col("col.rbat = dataSet.select(explode("batter").alias("batsman"))
bat = dataSet.select(explode("batter").alias("batsman"))
bat.show()
bowlers = dataSet.select(explode("bowler").alias("bowler"))
runs = dataSet.select(explode("runs").alias("runs"))
bat_rdd = bat.rdd
print(bat_rdd.collect())
bowler_rdd = bowlers.rdd
print(bowler_rdd.collect())
bat_rdd=bat.withColumn('row_index',row_number().over(Window.orderBy(monotonically_increasing_id())))
bowler_rdd=bowlers.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))
total = bowler_rdd.join(bat_rdd, on=["row_index"]).drop("row_index")
total.show()
runs_rdd=runs.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))
total=total.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))
total = total.join(runs_rdd, on=["row_index"]).drop("row_index")
total.summary().show()
total.show(400,False)
statDF =total.groupby(‘batsman’,’bowler’).sum(‘runs’).orderBy(‘batsman’)
statmaxDF = statDF.groupby("batsman","bowler").agg({"sum(runs)":"max"})
statmaxDF.show()
stat_max = statmaxDF.groupBy("batsman").max("max(sum(runs))") 
stat_max.show()
